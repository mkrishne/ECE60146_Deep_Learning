# -*- coding: utf-8 -*-
"""HW9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jcj0P-QhYWR4v_5tIyC-N2NmVxnu5-Fn
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/hw9_data

import csv

# Initialize empty lists to store sentences and sentiments
sentences = []
sentiments = []

# Open the CSV file for reading
with open('data.csv', 'r') as f:
    # Create a CSV reader object
    reader = csv.reader(f)

    # Skip the first line (header)
    next(reader)

    # Iterate through each row in the CSV
    for row in reader:
        # Append the sentence (first column) to the sentences list
        sentences.append(row[0])

        # Append the sentiment (second column) to the sentiments list
        sentiments.append(row[1])

# Tokenize the sentences word by word
word_tokenized_sentences = [sentence.split() for sentence in sentences]

# Find the maximum length of sentences
max_len = max([len(sentence) for sentence in word_tokenized_sentences])

# Pad the sentences with '[PAD]' token to make them of equal length
padded_sentences = [sentence + ['[PAD]'] * (max_len - len(sentence)) for sentence in word_tokenized_sentences]

from tabulate import tabulate
# Prepare table data
table_data_sentences = list(zip(sentences[:3], sentiments[:3]))
table_data_word_tokenized = word_tokenized_sentences[:3]
table_data_padded = padded_sentences[:3]

# Print the first 3 sentences and sentiments
print("First 3 sentences and sentiments:")
print(tabulate(table_data_sentences, headers=["Sentences", "Sentiments"], tablefmt="grid"))

# Print the first 3 word_tokenized_sentences
print("\nFirst 3 word_tokenized_sentences:")
print(tabulate(table_data_word_tokenized, tablefmt="grid"))

# Print the first 3 padded_sentences
print("\nFirst 3 padded_sentences:")
print(tabulate(table_data_padded, tablefmt="grid"))

from transformers import DistilBertTokenizer

# Define the pre-trained model checkpoint
model_ckpt = "distilbert-base-uncased"

# Initialize DistilBertTokenizer with the specified pre-trained model checkpoint
distilbert_tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)

# Encode the sentences using the DistilBert tokenizer, padding to max_length and truncating if necessary
bert_tokenized_sentences_ids = [distilbert_tokenizer.encode(
                                    sentence,
                                    padding='max_length',
                                    truncation=True,
                                    max_length=max_len
                                )
                                for sentence in sentences]

# Convert the encoded sentence IDs back to tokens using the tokenizer
bert_tokenized_sentences_tokens = [distilbert_tokenizer.convert_ids_to_tokens(sentence)
                                   for sentence in bert_tokenized_sentences_ids]

# Prepare table data
table_data = []

for ids, tokens in zip(bert_tokenized_sentences_ids[:3], bert_tokenized_sentences_tokens[:3]):
    max_length = max(len(ids), len(tokens))
    aligned_ids = ['ID'] + ids + [''] * (max_length - len(ids))
    aligned_tokens = ['Token'] + tokens + [''] * (max_length - len(tokens))
    table_data.append([aligned_ids, aligned_tokens])

# Print the aligned table data
for ids, tokens in table_data:
    print(tabulate([ids, tokens], tablefmt="grid"))
    print("\n" + "="*500 + "\n")

# Initialize an empty dictionary to store the vocabulary mapping
vocab = {}

# Assign token ID 0 to the special token '[PAD]'
vocab['[PAD]'] = 0

# Iterate over each sentence in the list of padded sentences
for sentence in padded_sentences:
    # Iterate over each token in the current sentence
    for token in sentence:
        # Check if the token is not already in the vocabulary
        if token not in vocab:
            # If the token is not in the vocabulary, assign a new token ID to it
            # The new token ID is determined by the current length of the vocabulary
            # This ensures each token gets a unique token ID
            vocab[token] = len(vocab)

# Print the vocabulary (optional)
# print(vocab)

# Convert the tokens to their corresponding token IDs using the vocabulary
# This generates a new list of lists where each token in each sentence is replaced by its token ID
padded_sentences_ids = [[vocab[token] for token in sentence] for sentence in padded_sentences]

from tabulate import tabulate

# Prepare table data
table_data = []

for sentence, sentence_ids, tokens in zip(padded_sentences[:3], padded_sentences_ids[:3], bert_tokenized_sentences_tokens[:3]):
    max_length = max(len(sentence), len(sentence_ids), len(tokens))
    aligned_sentence = ['Sentence'] + sentence + [''] * (max_length - len(sentence))
    aligned_sentence_ids = ['Sentence IDs'] + sentence_ids + [''] * (max_length - len(sentence_ids))
    aligned_tokens = ['Tokens'] + tokens + [''] * (max_length - len(tokens))
    table_data.append([aligned_sentence, aligned_sentence_ids, aligned_tokens])

# Print the aligned table data
for sentence, sentence_ids, tokens in table_data:
    print(tabulate([sentence, sentence_ids, tokens], tablefmt="grid"))
    print("\n" + "="*500 + "\n")

# Import necessary libraries
import torch
from transformers import DistilBertModel

# Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define the pre-trained DistilBERT model name
model_name = 'distilbert-base-uncased'

# Load the pre-trained DistilBERT model and move it to the appropriate device
distilbert_model = DistilBertModel.from_pretrained(model_name).to(device)

# Extract word embeddings
word_embeddings = []

# Loop through each sentence's tokens
for tokens in padded_sentences_ids:
    # Convert padded sentence tokens into tensors and move to device
    input_ids = torch.tensor(tokens).unsqueeze(0).to(device)

    # Disable gradient calculation for inference
    with torch.no_grad():
        # Forward pass the tokens through the DistilBERT model
        outputs = distilbert_model(input_ids)

    # Append the last hidden state (word embeddings) to the list
    word_embeddings.append(outputs.last_hidden_state)

# Print the shape of word embeddings for the first sentence
print(word_embeddings[0].shape)
# Output: torch.Size([1, 39, 768])

# Extract subword embeddings
subword_embeddings = []

# Loop through each sentence's tokenized IDs
for tokens in bert_tokenized_sentences_ids:
    # Convert tokenized IDs into tensors and move to device
    input_ids = torch.tensor(tokens).unsqueeze(0).to(device)

    # Disable gradient calculation for inference
    with torch.no_grad():
        # Forward pass the tokenized IDs through the DistilBERT model
        outputs = distilbert_model(input_ids)

    # Append the last hidden state (subword embeddings) to the list
    subword_embeddings.append(outputs.last_hidden_state)

# Print the shape of subword embeddings for the first sentence
print(subword_embeddings[0].shape)

print(len(subword_embeddings))

import seaborn as sns

# Plotting confusion matrix
def plot_conf_mat(conf_mat, classes, model_name):
    num_classes = len(classes)
    labels = [['' for _ in range(num_classes)] for _ in range(num_classes)]
    for row in range(num_classes):
        for col in range(num_classes):
            total_labels = np.sum(conf_mat[row])  # Sum along the row
            count = conf_mat[row][col]
            percent = "%.2f%%" % (count / total_labels * 100)
            label = f"{count}\n{percent}"
            labels[row][col] = label

    plt.figure(figsize=(6, 6))
    sns.heatmap(conf_mat, annot=labels, fmt="", cmap="YlOrBr", cbar=True,
                xticklabels=classes, yticklabels=classes)
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.title(f'Confusion Matrix for model {model_name}')
    plt.show()

import torch.nn as nn

class GRUnetWithEmbeddings(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1):
        super(GRUnetWithEmbeddings, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # Define GRU layer
        self.gru = nn.GRU(input_size, hidden_size, num_layers,dropout=0.2)

        # Define fully connected layer
        self.fc = nn.Linear(hidden_size, output_size)

        # Define activation function
        self.relu = nn.ReLU()

    def forward(self, x, h):
        # Forward pass through GRU layer
        out, h = self.gru(x, h)

        # Apply ReLU activation function
        out = self.fc(self.relu(out[-1]))

        return out, h

    def init_hidden(self):
        # Initialize hidden state with zeros
        weight = next(self.parameters()).data
        hidden = weight.new(self.num_layers, 1, self.hidden_size).zero_()
        return hidden

# Commented out IPython magic to ensure Python compatibility.
import torch.nn as nn
import torch.optim as optim
import copy
import time
import matplotlib.pyplot as plt
import os

# Routine to train GRU with embeddings
def train_gru_with_embeddings(device, net, dataloader, model_name, epochs, display_interval):
    #net = copy.deepcopy(net)
    net = net.to(device)

    criterion = nn.CrossEntropyLoss()
    accum_times = []
    optimizer = optim.Adam(net.parameters(), lr=1e-4, betas = (0.8, 0.999))
    training_loss_tally = []
    start_time = time.perf_counter()
    for epoch in range(epochs):
        running_loss = 0.0
        for i, data in enumerate(dataloader,0):
            review_tensor,sentiment = data
            review_tensor = review_tensor.to(device)
            sentiment = sentiment.to(device)

            optimizer.zero_grad()
            hidden = net.init_hidden().to(device)
            output, hidden = net(torch.unsqueeze(review_tensor[0], 1), hidden)
            loss = criterion(output, torch.argmax(sentiment, 1))
            running_loss += loss.item()
            loss.backward()
            optimizer.step()

            if (i+1) % display_interval == 0:
                avg_loss = running_loss / float(display_interval)
                training_loss_tally.append(avg_loss)
                current_time = time.perf_counter()
                time_elapsed = current_time-start_time
                print("[epoch:%d  iter:%4d  elapsed_time:%4d secs] loss: %.5f" % (epoch+1,i+1, time_elapsed,avg_loss))
                accum_times.append(current_time-start_time)
                running_loss = 0.0

    # Save model weights
    checkpoint_path = os.path.join('/content/drive/My Drive/hw9_data/saved_models',
                                   f'{model_name}.pt')
    torch.save(net.state_dict(), checkpoint_path)

    print("Total Training Time: {}".format(str(sum(accum_times))))
    print("\nFinished Training\n\n")
    plt.figure(figsize=(10,5))
    plt.title(f"Training Loss vs. Iterations - {model_name}")
    plt.plot(training_loss_tally)
    plt.xlabel("Iterations")
    plt.ylabel("Training loss")
    plt.legend()
    plt.savefig(f"/content/drive/My Drive/hw9_data/training_loss_{model_name}.png")
    plt.show()

    return training_loss_tally

len(sentiments)

import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split

# we have a dataset named 'word_embeddings' and 'sentiments' containing word embeddings and corresponding sentiments

# Step 1: Split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(word_embeddings, sentiments, test_size=0.2, random_state=42)

import numpy as np
from sklearn.preprocessing import OneHotEncoder

# Convert y_train and y_test to numpy arrays
y_train = np.array(y_train).reshape(-1, 1)
y_test = np.array(y_test).reshape(-1, 1)

# Initialize the OneHotEncoder
encoder = OneHotEncoder()

# Fit and transform on training labels
y_train_encoded = encoder.fit_transform(y_train).toarray()

# Print encoded training labels
print("Encoded training labels:")
for label, encoded_label in zip(y_train, y_train_encoded):
    print(f"Original label: {label}, Encoded label: {encoded_label}")

# Transform test labels
y_test_encoded = encoder.transform(y_test).toarray()

# Print encoded test labels
print("\nEncoded test labels:")
for label, encoded_label in zip(y_test, y_test_encoded):
    print(f"Original label: {label}, Encoded label: {encoded_label}")

import torch
from torch.utils.data import Dataset, DataLoader

# Step 2: Define a custom dataset class
class CustomDataset(Dataset):
    def __init__(self, word_embeddings, sentiments):
        self.word_embeddings = word_embeddings
        self.sentiments = sentiments

    def __len__(self):
        return len(self.word_embeddings)

    def __getitem__(self, idx):
        return self.word_embeddings[idx], self.sentiments[idx]

# Step 3: Implement the custom collate function to handle batching
def custom_collate_fn(batch):
    embeddings, sentiments = zip(*batch)
    embeddings = torch.stack([torch.squeeze(embedding) for embedding in embeddings])  # Remove dimensions with size 1
    sentiments = torch.tensor(sentiments)  # Convert list to tensor with integer type
    return embeddings, sentiments

# Step 4: Create custom data loaders for the train and test sets
train_dataset = CustomDataset(X_train, y_train_encoded)
test_dataset = CustomDataset(X_test, y_test_encoded)

# Create custom data loaders for the train and test sets
train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn)

# Define a function to convert one-hot encoded tensors to sentiment labels
def decode_sentiments(sentiments):
    sentiment_labels = []
    for sentiment in sentiments:
        if sentiment[0] == 1:
            sentiment_labels.append("Negative")
        elif sentiment[1] == 1:
            sentiment_labels.append("Neutral")
        elif sentiment[2] == 1:
            sentiment_labels.append("Positive")
        else:
            sentiment_labels.append("Unknown")
    return sentiment_labels

# Print a sample batch from the train data loader along with its size and sentiment labels
train_batch = next(iter(train_loader))
train_embeddings, train_sentiments = train_batch
train_sentiment_labels = decode_sentiments(train_sentiments)

print("Sample batch from train data loader:")
print("Embeddings:")
print(train_embeddings)
print("Sentiments:")
print(train_sentiment_labels)
print("Size of train batch:")
print("Embeddings:", train_embeddings.size())
print("Sentiments:", train_sentiments.size())

# Print a sample batch from the test data loader along with its size and sentiment labels
test_batch = next(iter(test_loader))
test_embeddings, test_sentiments = test_batch
test_sentiment_labels = decode_sentiments(test_sentiments)

print("\nSample batch from test data loader:")
print("Embeddings:")
print(test_embeddings)
print("Sentiments:")
print(test_sentiment_labels)
print("Size of test batch:")
print("Embeddings:", test_embeddings.size())
print("Sentiments:", test_sentiments.size())

len(sentiments)

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize the model
input_size = 768
hidden_size = 100  # 128 hidden size
output_size = 3  # Assuming 3 classes for sentiment analysis
num_layers = 3  # Choosing 2 layers
net = GRUnetWithEmbeddings(input_size, hidden_size, output_size, num_layers)

# Train the model
epochs = 5
display_interval = 500
model_name = "sentiment_analysis_model2"
train_loss = train_gru_with_embeddings(device, net, train_loader, model_name, epochs, display_interval)

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize the model
input_size = 768
hidden_size = 100  # 128 hidden size
output_size = 3  # Assuming 3 classes for sentiment analysis
num_layers = 3  # Choosing 2 layers
net = GRUnetWithEmbeddings(input_size, hidden_size, output_size, num_layers)

# Train the model
epochs = 5
display_interval = 500
model_name = "sentiment_analysis_model"
train_loss = train_gru_with_embeddings(device, net, train_loader, model_name, epochs, display_interval)

import torch
import numpy as np

def validate_gru_with_embeddings(device, net, model_path, dataloader, model_name, display_interval):
    # Load model weights
    net.load_state_dict(torch.load(model_path))

    # Move the network to the specified device
    net = net.to(device)

    # Initialize variables for accuracy calculation
    classification_accuracy = 0.0
    positive_total = 0
    negative_total = 0
    neutral_total = 0

    # Initialize confusion matrix
    confusion_matrix = torch.zeros(3, 3)

    # Turn off gradients
    with torch.no_grad():
        for i, data in enumerate(dataloader, 0):
            review_tensor, sentiment = data

            # Move tensors to device
            review_tensor = review_tensor.to(device)
            sentiment = sentiment.to(device)

            # Initialize hidden state
            hidden = net.init_hidden().to(device)

            # Forward pass
            output, hidden = net(torch.unsqueeze(review_tensor[0], 1), hidden)

            # Predicted and ground truth indices
            _, predicted_idx = torch.max(output, 1)
            gt_idx = torch.argmax(sentiment, 1)

            # Update confusion matrix directly using predicted_idx and gt_idx
            confusion_matrix[gt_idx.item(), predicted_idx.item()] += 1

            # Calculate accuracy
            classification_accuracy += torch.sum(predicted_idx == gt_idx).item()

            # Print progress every display_interval iterations
            if (i + 1) % display_interval == 0:
                print(f" [i={i + 1}]")

        # Print overall classification accuracy
        total_samples = len(dataloader.dataset)
        accuracy = classification_accuracy / total_samples * 100
        print("\nOverall classification accuracy: %0.2f%%" % accuracy)

        # Display the confusion matrix
        print("\nDisplaying the confusion matrix:\n")
        classes = ['negative', 'neutral', 'positive']
        print(confusion_matrix)
        plot_conf_mat(confusion_matrix.numpy(), classes, model_name)

# Make sure you have implemented the plot_conf_mat function for plotting the confusion matrix.

# Validate the model
model_path = "/content/drive/My Drive/hw9_data/saved_models/sentiment_analysis_model2.pt"
display_interval_val = 100
validate_gru_with_embeddings(device, net, model_path, test_loader, model_name, display_interval_val)

# Bidirectional torch.nn GRU with Embeddings
class BiGRUnetWithEmbeddings(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1):
        super(BiGRUnetWithEmbeddings, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.gru = nn.GRU(input_size, hidden_size, num_layers, bidirectional=True,dropout=0.2)
        self.fc = nn.Linear(2*hidden_size, output_size)
        self.relu = nn.ReLU()

    def forward(self, x, h):
        out, h = self.gru(x, h)
        out = self.fc(self.relu(out[-1]))
        return out, h

    def init_hidden(self):
        weight = next(self.parameters()).data
        #                  num_layers  batch_size    hidden_size
        hidden = weight.new(  2*self.num_layers,          1,         self.hidden_size    ).zero_()
        return hidden

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize the model
input_size = 768
hidden_size = 100  # 128 hidden size
output_size = 3  # Assuming 3 classes for sentiment analysis
num_layers = 2  # Choosing 2 layers
net = BiGRUnetWithEmbeddings(input_size, hidden_size, output_size, num_layers)

# Train the model
epochs = 5
display_interval = 500
model_name = "sentiment_analysis_model_bidir"
train_loss = train_gru_with_embeddings(device, net, train_loader, model_name, epochs, display_interval)

# Validate the model
model_path = "/content/drive/My Drive/hw9_data/saved_models/sentiment_analysis_model_bidir.pt"
display_interval_val = 100
validate_gru_with_embeddings(device, net, model_path, test_loader, model_name, display_interval_val)

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize the model
input_size = 768
hidden_size = 200  # 128 hidden size
output_size = 3  # Assuming 3 classes for sentiment analysis
num_layers = 3  # Choosing 2 layers
net = BiGRUnetWithEmbeddings(input_size, hidden_size, output_size, num_layers)

# Train the model
epochs = 5
display_interval = 500
model_name = "sentiment_analysis_model_bidir_3lay"
train_loss = train_gru_with_embeddings(device, net, train_loader, model_name, epochs, display_interval)

# Validate the model
model_path = "/content/drive/My Drive/hw9_data/saved_models/sentiment_analysis_model_bidir_3lay.pt"
display_interval_val = 100
validate_gru_with_embeddings(device, net, model_path, test_loader, model_name, display_interval_val)

import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split

# wehave a dataset named 'word_embeddings' and 'sentiments' containing word embeddings and corresponding sentiments

# Step 1: Split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(subword_embeddings, sentiments, test_size=0.2, random_state=42)

import numpy as np
from sklearn.preprocessing import OneHotEncoder

# Convert y_train and y_test to numpy arrays
y_train = np.array(y_train).reshape(-1, 1)
y_test = np.array(y_test).reshape(-1, 1)

# Initialize the OneHotEncoder
encoder = OneHotEncoder()

# Fit and transform on training labels
y_train_encoded = encoder.fit_transform(y_train).toarray()

# Print encoded training labels
print("Encoded training labels:")
for label, encoded_label in zip(y_train, y_train_encoded):
    print(f"Original label: {label}, Encoded label: {encoded_label}")

# Transform test labels
y_test_encoded = encoder.transform(y_test).toarray()

# Print encoded test labels
print("\nEncoded test labels:")
for label, encoded_label in zip(y_test, y_test_encoded):
    print(f"Original label: {label}, Encoded label: {encoded_label}")

# Step 4: Create custom data loaders for the train and test sets
train_dataset = CustomDataset(X_train, y_train_encoded)
test_dataset = CustomDataset(X_test, y_test_encoded)

# Create custom data loaders for the train and test sets
train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn)

# Define a function to convert one-hot encoded tensors to sentiment labels
def decode_sentiments(sentiments):
    sentiment_labels = []
    for sentiment in sentiments:
        if sentiment[0] == 1:
            sentiment_labels.append("Negative")
        elif sentiment[1] == 1:
            sentiment_labels.append("Neutral")
        elif sentiment[2] == 1:
            sentiment_labels.append("Positive")
        else:
            sentiment_labels.append("Unknown")
    return sentiment_labels

# Print a sample batch from the train data loader along with its size and sentiment labels
train_batch = next(iter(train_loader))
train_embeddings, train_sentiments = train_batch
train_sentiment_labels = decode_sentiments(train_sentiments)

print("Sample batch from train data loader:")
print("Subword Embeddings:")
print(train_embeddings)
print("Sentiments:")
print(train_sentiment_labels)
print("Size of train batch:")
print("Subword Embeddings:", train_embeddings.size())
print("Sentiments:", train_sentiments.size())

# Print a sample batch from the test data loader along with its size and sentiment labels
test_batch = next(iter(test_loader))
test_embeddings, test_sentiments = test_batch
test_sentiment_labels = decode_sentiments(test_sentiments)

print("\nSample batch from test data loader:")
print("Subword Embeddings:")
print(test_embeddings)
print("Sentiments:")
print(test_sentiment_labels)
print("Size of test batch:")
print("Subword Embeddings Size:", test_embeddings.size())
print("Sentiments:", test_sentiments.size())

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize the model
input_size = 768
hidden_size = 100  # 128 hidden size
output_size = 3  # Assuming 3 classes for sentiment analysis
num_layers = 3  # Choosing 2 layers
net = GRUnetWithEmbeddings(input_size, hidden_size, output_size, num_layers)

# Train the model
epochs = 10
display_interval = 500
model_name = "sentiment_analysis_model_subword"
train_loss = train_gru_with_embeddings(device, net, train_loader, model_name, epochs, display_interval)

# Validate the model
model_path = "/content/drive/My Drive/hw9_data/saved_models/sentiment_analysis_model_subword.pt"
display_interval_val = 100
validate_gru_with_embeddings(device, net, model_path, test_loader, model_name, display_interval_val)

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize the model
input_size = 768
hidden_size = 100  # 128 hidden size
output_size = 3  # Assuming 3 classes for sentiment analysis
num_layers = 3  # Choosing 2 layers
net = BiGRUnetWithEmbeddings(input_size, hidden_size, output_size, num_layers)

# Train the model
epochs = 15
display_interval = 500
model_name = "sentiment_analysis_model_bidir_subword"
train_loss = train_gru_with_embeddings(device, net, train_loader, model_name, epochs, display_interval)

# Validate the model
model_path = "/content/drive/My Drive/hw9_data/saved_models/sentiment_analysis_model_bidir_subword.pt"
display_interval_val = 100
validate_gru_with_embeddings(device, net, model_path, test_loader, model_name, display_interval_val)