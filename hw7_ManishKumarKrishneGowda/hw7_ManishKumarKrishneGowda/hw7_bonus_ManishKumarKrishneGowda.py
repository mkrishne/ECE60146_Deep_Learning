# -*- coding: utf-8 -*-
"""HW7_bonus.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-hJz7C89hKYslFc1FJSs_p-7ucPKzRs2
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/hw6_data/

!pip install pymsgbox

import os
import torch
import random
import numpy as np
import requests
import matplotlib.pyplot as plt
import tqdm
from PIL import Image
from pycocotools.coco import COCO

seed = 0
random.seed(seed)
np.random.seed(seed)

class ImageDownloader():
    def __init__(
        self, root_dir, annotation_path, classes, min_annotation_area=40000  # 200x200 = 40000 pixels
    ):
        self.root_dir = root_dir
        self.annotation_path = annotation_path
        self.classes = classes
        self.min_annotation_area = min_annotation_area

        self.coco = COCO(annotation_path)
        self.catIds = self.coco.getCatIds(catNms=classes)
        self.categories = self.coco.loadCats(self.catIds)
        self.categories.sort(key=lambda x: x["id"])
        self.class_dir = {}

        self.coco_labels_inverse = {
            c["id"]: idx for idx, c in enumerate(self.categories)
        }

    def create_directories(self):
        for c in self.classes:
            dir_path = os.path.join(self.root_dir, c)
            self.class_dir[c] = dir_path
            os.makedirs(dir_path, exist_ok=True)

    def download_images(self, download=True, val=False):
        img_paths = {c: [] for c in self.classes}
        img_masks = {c: [] for c in self.classes}

        for c in tqdm.tqdm(self.classes):
            class_id = self.coco.getCatIds(c)
            img_ids = self.coco.getImgIds(catIds=class_id)
            images = self.coco.loadImgs(img_ids)

            for image in images:
                annIds = self.coco.getAnnIds(
                    imgIds=image["id"], catIds=class_id, iscrowd=False
                )
                annotations = self.coco.loadAnns(annIds)

                valid_annotations = [
                    ann
                    for ann in annotations
                    if ann["area"] >= self.min_annotation_area
                ]

                if len(valid_annotations) == 1:
                    ann = valid_annotations[0]
                    mask = self.coco.annToMask(ann)
                    mask_path = os.path.join(
                        self.root_dir, c, image["file_name"].replace(".jpg", "_mask.png")
                    )
                    # Convert mask array to binary mask with 0 for background and 1 for mask
                    binary_mask = (mask > 0).astype(np.uint8) * 255

                    # Create PIL image from the binary mask and save it
                    Image.fromarray(binary_mask, mode='L').save(mask_path)

                    img_path = os.path.join(self.root_dir, c, image["file_name"])
                    if download:
                        if self.download_image(img_path, image["coco_url"]):
                            self.convert_image(img_path)
                            img_paths[c].append(img_path)
                            img_masks[c].append(mask_path)
                    else:
                        img_paths[c].append(img_path)
                        img_masks[c].append(mask_path)

        return img_paths, img_masks

    # Download image from URL using requests
    def download_image(self, path, url):
        try:
            img_data = requests.get(url).content
            with open(path, "wb") as f:
                f.write(img_data)
            return True
        except Exception as e:
            print(f"Caught exception: {e}")
        return False

    # Convert image
    def convert_image(self, path):
        im = Image.open(path)
        if im.mode != "RGB":
            im = im.convert(mode="RGB")
        im.save(path)

import matplotlib.pyplot as plt
from typing import Dict, List
classes = ['cake', 'dog', 'motorcycle']
try:
    # Download training images
    train_downloader = ImageDownloader('/content/drive/MyDrive/hw6_data/coco/train2017',
                '/content/drive/MyDrive/hw6_data/coco/annotations/instances_train2017.json',
                classes)

    train_downloader.create_directories()
    train_img_paths, train_img_masks = train_downloader.download_images(download=False)

    # Access and process downloaded data
    num_cake_images = len(train_img_paths["cake"])
    num_dog_images = len(train_img_paths["dog"])
    num_mc_images = len(train_img_paths["motorcycle"])

    print()
    print(f"Number of downloaded 'cake' images: {num_cake_images}")
    print(f"Number of downloaded 'dog' images: {num_dog_images}")
    print(f"Number of downloaded 'motorcycle' images: {num_mc_images}")

     # Load and display sample images and masks for cake, dog, and motorcycle
    for category in classes:
        num_images = len(train_img_paths[category])

        if num_images > 0:
            sample_image_path = train_img_paths[category][0]  # Assuming valid index
            sample_mask_path = train_img_masks[category][0]  # Assuming valid index

            # Load and display image
            sample_image = Image.open(sample_image_path)
            plt.figure(figsize=(8, 4))
            plt.subplot(1, 2, 1)
            plt.imshow(sample_image)
            plt.title(f'{category.capitalize()} Image')

            # Load and display mask
            sample_mask = Image.open(sample_mask_path)
            plt.subplot(1, 2, 2)
            plt.imshow(sample_mask, cmap='gray')
            plt.title(f'{category.capitalize()} Mask')

            plt.show()
            print("-" * 40)  # Optional separator

except Exception as e:
    print(f"An error occurred: {e}")

import matplotlib.pyplot as plt
from typing import Dict, List
classes = ['cake', 'dog', 'motorcycle']
try:
    # Download valing images
    val_downloader = ImageDownloader('/content/drive/MyDrive/hw6_data/coco/val2017',
                '/content/drive/MyDrive/hw6_data/coco/annotations/instances_val2017.json',
                classes)

    val_downloader.create_directories()
    val_img_paths, val_img_masks = val_downloader.download_images(download=False)

    # Access and process downloaded data
    num_cake_images = len(val_img_paths["cake"])
    num_dog_images = len(val_img_paths["dog"])
    num_mc_images = len(val_img_paths["motorcycle"])

    print()
    print(f"Number of downloaded 'cake' images: {num_cake_images}")
    print(f"Number of downloaded 'dog' images: {num_dog_images}")
    print(f"Number of downloaded 'motorcycle' images: {num_mc_images}")

     # Load and display sample images and masks for cake, dog, and motorcycle
    for category in classes:
        num_images = len(val_img_paths[category])

        if num_images > 0:
            sample_image_path = val_img_paths[category][0]  # Assuming valid index
            sample_mask_path = val_img_masks[category][0]  # Assuming valid index

            # Load and display image
            sample_image = Image.open(sample_image_path)
            plt.figure(figsize=(8, 4))
            plt.subplot(1, 2, 1)
            plt.imshow(sample_image)
            plt.title(f'{category.capitalize()} Validation Image')

            # Load and display mask
            sample_mask = Image.open(sample_mask_path)
            plt.subplot(1, 2, 2)
            plt.imshow(sample_mask, cmap='gray')
            plt.title(f'{category.capitalize()} Validation Mask')

            plt.show()
            print("-" * 40)  # Optional separator

except Exception as e:
    print(f"An error occurred: {e}")

import os
from PIL import Image

def resize_images(directory_path, save_directory):
    # List all files in the directory
    files = os.listdir(directory_path)

    # Create the save directory if it doesn't exist
    os.makedirs(save_directory, exist_ok=True)

    # Loop through each file
    for file in files:
        # Check if the file is an image (you can add more image formats if needed)
        if file.endswith('.jpg') or file.endswith('.png'):
            try:
                # Open the image file
                image_path = os.path.join(directory_path, file)
                img = Image.open(image_path)

                # Resize the image to 256x256 pixels
                resized_img = img.resize((64, 64))

                # Save the resized image with the same filename in the new directory
                save_path = os.path.join(save_directory, file)
                resized_img.save(save_path)

                #print(f"Resized and saved: {file}")

            except Exception as e:
                print(f"Error processing {file}: {e}")
    print("done resizing")

# Specify the main directory containing the images
main_directory_path = '/content/drive/MyDrive/hw6_data/coco/'
dir_types = ["val2017/","train2017/"]
classes = ['cake', 'dog', 'motorcycle']

# Loop through each directory type (val2017 and train2017)
for dir_name in dir_types:
    # Loop through each class
    for class_name in classes:
        # Define the source directory path and the destination save directory path
        source_directory_path = os.path.join(main_directory_path, dir_name, class_name)
        save_directory = os.path.join(main_directory_path, dir_name[:-1] + "_resized/", class_name)

        # Resize images and save them in the new directory
        resize_images(source_directory_path, save_directory)

# Define the main directory path
main_directory_path = '/content/drive/MyDrive/hw6_data/coco/'

# Define the directory types
dir_types = ["train2017_resized/"]

# Define the classes
classes = ['cake', 'dog', 'motorcycle']

# Initialize an empty dictionary to store the file paths
train_img_paths = {class_name: [] for class_name in classes}
train_img_masks = {class_name: [] for class_name in classes}

# Loop through each directory type
for dir_name in dir_types:
    # Loop through each class
    for class_name in classes:
        # Construct the directory path
        directory_path = os.path.join(main_directory_path, dir_name, class_name)
        # List all files in the directory
        files = os.listdir(directory_path)
        # Iterate over each file and append its path to the corresponding class list
        for file in files:
            file_path = os.path.join(directory_path, file)
            if file.endswith('.jpg'):
              train_img_paths[class_name].append(file_path)
            else:
              train_img_masks[class_name].append(file_path)

# Define the main directory path
main_directory_path = '/content/drive/MyDrive/hw6_data/coco/'

# Define the directory types
dir_types = ["val2017_resized/"]

# Define the classes
classes = ['cake', 'dog', 'motorcycle']

# Initialize an empty dictionary to store the file paths
val_img_paths = {class_name: [] for class_name in classes}
val_img_masks = {class_name: [] for class_name in classes}

# Loop through each directory type
for dir_name in dir_types:
    # Loop through each class
    for class_name in classes:
        # Construct the directory path
        directory_path = os.path.join(main_directory_path, dir_name, class_name)
        # List all files in the directory
        files = os.listdir(directory_path)
        # Iterate over each file and append its path to the corresponding class list
        for file in files:
            file_path = os.path.join(directory_path, file)
            if file.endswith('.jpg'):
              val_img_paths[class_name].append(file_path)
            else:
              val_img_masks[class_name].append(file_path)

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/hw6_data/DLStudio-2.3.6
from DLStudio import *
import torch.nn as nn
import copy
import time
import torch.optim as optim
import torchvision
import gzip
import pickle
import matplotlib.pyplot as plt

class SemanticSegmentation(nn.Module):
    def __init__(self, dl_studio, dataserver_train=None, dataserver_test=None, dataset_file_train=None, dataset_file_test=None):
        super(SemanticSegmentation, self).__init__()
        self.dl_studio = dl_studio
        self.dataserver_train = dataserver_train
        self.dataserver_test = dataserver_test

    class CocoDataset(torch.utils.data.Dataset):
        def __init__(self, dl_studio, class_labels, img_paths, img_masks, mode = 'train'):
            super().__init__()
            self.mode = mode
            self.class_labels = class_labels
            self.num_shapes = len(self.class_labels)
            self.image_size = dl_studio.image_size

            self.img_paths = []
            self.img_masks = []
            # Populate lists with data from provided paths and annotations
            for cls in self.class_labels:
                self.img_paths += img_paths[cls]
                self.img_masks += img_masks[cls]

        def __len__(self):
            # Return the total number of images
            return len(self.img_paths)

        def __getitem__(self, idx):
            idx = idx % len(self.img_paths)
            img_path = self.img_paths[idx]
            image_size = self.image_size
            im = Image.open(img_path).convert('RGB')
            r, g, b = im.split()
            # Convert each channel to numpy arrays
            R = np.array(r)
            G = np.array(g)
            B = np.array(b)

            # Reshape the arrays to match the desired image size
            R = R.reshape(image_size)
            G = G.reshape(image_size)
            B = B.reshape(image_size)
            im_tensor = torch.zeros(3,image_size[0],image_size[1], dtype=torch.float)
            im_tensor[0,:,:] = torch.from_numpy(R)
            im_tensor[1,:,:] = torch.from_numpy(G)
            im_tensor[2,:,:] = torch.from_numpy(B)

            file_name = os.path.basename(img_path)
            file_name_without_extension = os.path.splitext(file_name)[0]
            img_mask = os.path.join(os.path.dirname(img_path), f"{file_name_without_extension}_mask.png")
            mask_im = Image.open(img_mask)
            mask_im = mask_im.convert('L')
            mask_array = np.array(mask_im)
            mask_array = mask_array.reshape(image_size)
            mask_tensor = torch.from_numpy(mask_array)
            sample = {'image'        : im_tensor,
                'mask_tensor'  : mask_tensor}
            return sample

    def load_CocoDataset(self, dataserver_train, dataserver_test ):
        self.train_dataloader = torch.utils.data.DataLoader(dataserver_train,
                    batch_size=self.dl_studio.batch_size,shuffle=True, num_workers=4)
        self.test_dataloader = torch.utils.data.DataLoader(dataserver_test,
                            batch_size=self.dl_studio.batch_size,shuffle=False, num_workers=4)

    class SkipBlockDN(nn.Module):
        def __init__(self, in_ch, out_ch, downsample=False, skip_connections=True):
            super(SemanticSegmentation.SkipBlockDN, self).__init__()
            self.downsample = downsample
            self.skip_connections = skip_connections
            self.in_ch = in_ch
            self.out_ch = out_ch
            self.convo1 = nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1)
            self.convo2 = nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1)
            self.bn1 = nn.BatchNorm2d(out_ch)
            self.bn2 = nn.BatchNorm2d(out_ch)
            if downsample:
                self.downsampler = nn.Conv2d(in_ch, out_ch, 1, stride=2)
        def forward(self, x):
            identity = x
            out = self.convo1(x)
            out = self.bn1(out)
            out = nn.functional.relu(out)
            if self.in_ch == self.out_ch:
                out = self.convo2(out)
                out = self.bn2(out)
                out = nn.functional.relu(out)
            if self.downsample:
                out = self.downsampler(out)
                identity = self.downsampler(identity)
            if self.skip_connections:
                if self.in_ch == self.out_ch:
                    out = out + identity
                else:
                    out = out + torch.cat((identity, identity), dim=1)
            return out


    class SkipBlockUP(nn.Module):
        def __init__(self, in_ch, out_ch, upsample=False, skip_connections=True):
            super(SemanticSegmentation.SkipBlockUP, self).__init__()
            self.upsample = upsample
            self.skip_connections = skip_connections
            self.in_ch = in_ch
            self.out_ch = out_ch
            self.convoT1 = nn.ConvTranspose2d(in_ch, out_ch, 3, padding=1)
            self.convoT2 = nn.ConvTranspose2d(in_ch, out_ch, 3, padding=1)
            self.bn1 = nn.BatchNorm2d(out_ch)
            self.bn2 = nn.BatchNorm2d(out_ch)
            if upsample:
                self.upsampler = nn.ConvTranspose2d(in_ch, out_ch, 1, stride=2, dilation=2, output_padding=1, padding=0)
        def forward(self, x):
            identity = x
            out = self.convoT1(x)
            out = self.bn1(out)
            out = nn.functional.relu(out)
            out  =  nn.ReLU(inplace=False)(out)
            if self.in_ch == self.out_ch:
                out = self.convoT2(out)
                out = self.bn2(out)
                out = nn.functional.relu(out)
            if self.upsample:
                out = self.upsampler(out)
                identity = self.upsampler(identity)
            if self.skip_connections:
                if self.in_ch == self.out_ch:
                    out = out + identity
                else:
                    out = out + identity[:,self.out_ch:,:,:]
            return out


    class mUnet(nn.Module):
        def __init__(self, skip_connections=True, depth=16):
            super(SemanticSegmentation.mUnet, self).__init__()
            self.depth = depth // 2
            self.conv_in = nn.Conv2d(3, 64, 3, padding=1)
            ##  For the DN arm of the U:
            self.bn1DN  = nn.BatchNorm2d(64)
            self.bn2DN  = nn.BatchNorm2d(128)
            self.skip64DN_arr = nn.ModuleList()
            for i in range(self.depth):
                self.skip64DN_arr.append(SemanticSegmentation.SkipBlockDN(64, 64, skip_connections=skip_connections))
            self.skip64dsDN = SemanticSegmentation.SkipBlockDN(64, 64,   downsample=True, skip_connections=skip_connections)
            self.skip64to128DN = SemanticSegmentation.SkipBlockDN(64, 128, skip_connections=skip_connections )
            self.skip128DN_arr = nn.ModuleList()
            for i in range(self.depth):
                self.skip128DN_arr.append(SemanticSegmentation.SkipBlockDN(128, 128, skip_connections=skip_connections))
            self.skip128dsDN = SemanticSegmentation.SkipBlockDN(128,128, downsample=True, skip_connections=skip_connections)
            ##  For the UP arm of the U:
            self.bn1UP  = nn.BatchNorm2d(128)
            self.bn2UP  = nn.BatchNorm2d(64)
            self.skip64UP_arr = nn.ModuleList()
            for i in range(self.depth):
                self.skip64UP_arr.append(SemanticSegmentation.SkipBlockUP(64, 64, skip_connections=skip_connections))
            self.skip64usUP = SemanticSegmentation.SkipBlockUP(64, 64, upsample=True, skip_connections=skip_connections)
            self.skip128to64UP = SemanticSegmentation.SkipBlockUP(128, 64, skip_connections=skip_connections )
            self.skip128UP_arr = nn.ModuleList()
            for i in range(self.depth):
                self.skip128UP_arr.append(SemanticSegmentation.SkipBlockUP(128, 128, skip_connections=skip_connections))
            self.skip128usUP = SemanticSegmentation.SkipBlockUP(128,128, upsample=True, skip_connections=skip_connections)
            self.conv_out = nn.ConvTranspose2d(64, 1, 3, stride=2,dilation=2,output_padding=1,padding=2)

        def forward(self, x):
            ##  Going down to the bottom of the U:
            x = nn.MaxPool2d(2,2)(nn.functional.relu(self.conv_in(x)))
            for i,skip64 in enumerate(self.skip64DN_arr[:self.depth//4]):
                x = skip64(x)

            num_channels_to_save1 = x.shape[1] // 2
            save_for_upside_1 = x[:,:num_channels_to_save1,:,:].clone()
            x = self.skip64dsDN(x)
            for i,skip64 in enumerate(self.skip64DN_arr[self.depth//4:]):
                x = skip64(x)
            x = self.bn1DN(x)
            num_channels_to_save2 = x.shape[1] // 2
            save_for_upside_2 = x[:,:num_channels_to_save2,:,:].clone()
            x = self.skip64to128DN(x)
            for i,skip128 in enumerate(self.skip128DN_arr[:self.depth//4]):
                x = skip128(x)

            x = self.bn2DN(x)
            num_channels_to_save3 = x.shape[1] // 2
            save_for_upside_3 = x[:,:num_channels_to_save3,:,:].clone()
            for i,skip128 in enumerate(self.skip128DN_arr[self.depth//4:]):
                x = skip128(x)
            x = self.skip128dsDN(x)
            ## Coming up from the bottom of U on the other side:
            x = self.skip128usUP(x)
            for i,skip128 in enumerate(self.skip128UP_arr[:self.depth//4]):
                x = skip128(x)
            x[:,:num_channels_to_save3,:,:] =  save_for_upside_3
            x = self.bn1UP(x)
            for i,skip128 in enumerate(self.skip128UP_arr[:self.depth//4]):
                x = skip128(x)
            x = self.skip128to64UP(x)
            for i,skip64 in enumerate(self.skip64UP_arr[self.depth//4:]):
                x = skip64(x)
            x[:,:num_channels_to_save2,:,:] =  save_for_upside_2
            x = self.bn2UP(x)
            x = self.skip64usUP(x)
            for i,skip64 in enumerate(self.skip64UP_arr[:self.depth//4]):
                x = skip64(x)
            x[:,:num_channels_to_save1,:,:] =  save_for_upside_1
            x = self.conv_out(x)
            return x

    class SegmentationLossMSE(nn.Module):
        """
        I wrote this class before I switched to MSE loss.  I am leaving it here
        in case I need to get back to it in the future.

        Class Path:   DLStudio  ->  SemanticSegmentation  ->  SegmentationLoss
        """
        def __init__(self, batch_size):
            super(SemanticSegmentation.SegmentationLossMSE, self).__init__()
            self.batch_size = batch_size
            #print(self.batch_size)
        def forward(self, output, mask_tensor):
            #print("output shape "+ str(output.size()))
            #print("mask_tensor shape "+ str(mask_tensor.size()))
            composite_loss = torch.zeros(1,output.shape[0])
            for idx in range(output.shape[0]):
                #print("idx =" + str(idx))
                outputh = output[idx,:,:]
                #print("outputh shape " + str(outputh.size()))
                mask = mask_tensor[idx,:,:]
                #print("mask shape " + str(mask.size()))
                element_wise = torch.abs(outputh - mask)
                #print("element_wise " + str(torch.mean(element_wise)))
                composite_loss[0,idx] = torch.mean(element_wise)
                #print("composite loss " + str(torch.sum(composite_loss) / output.shape[0]))
            return torch.sum(composite_loss) / output.shape[0]

    class SegmentationLossDice(nn.Module):
        def __init__(self, batch_size):
            super(SemanticSegmentation.SegmentationLossDice, self).__init__()
            self.batch_size = batch_size
        def forward(self, output, mask_tensor):
            composite_loss = torch.zeros(1,output.shape[0])
            mask_based_loss = torch.zeros(1,1)
            for idx in range(output.shape[0]):
                #outputh = output[idx,0,:,:]
                #print("output shape " + str(output.size()))
                outputh = output[idx,:,:]
                #print("outputh shape " + str(outputh.size()))
                mask = mask_tensor[idx,:,:]
                #print("mask shape " + str(mask.size()))
                numerator = torch.sum(2*(outputh*mask))
                denominator = torch.sum(outputh*outputh + mask*mask + 1)
                dice_loss = 1-numerator/denominator
                composite_loss[0,idx] = torch.mean(dice_loss)
            return torch.sum(composite_loss) / output.shape[0]

    class SegmentationLossComb(nn.Module):
        def __init__(self, batch_size,dice_scale):
            super(SemanticSegmentation.SegmentationLossComb, self).__init__()
            self.batch_size = batch_size
            self.loss_scaling_factor = dice_scale
        def forward(self, output, mask_tensor):
            dice_loss = SemanticSegmentation.SegmentationLossDice(self.batch_size)(output, mask_tensor)
            mse_loss = SemanticSegmentation.SegmentationLossMSE(self.batch_size)(output, mask_tensor)
            return self.loss_scaling_factor * dice_loss + mse_loss

    def custom_run_code_for_training_for_semantic_segmentation(self, net, loss_type, dice_scale=0):
        filename_for_out1 = "performance_numbers_" + str(self.dl_studio.epochs) + ".txt"
        FILE1 = open(filename_for_out1, 'w')
        net = copy.deepcopy(net)
        net = net.to(self.dl_studio.device)
        if loss_type == "MSE":
            #criterion1 = nn.MSELoss()
            criterion1 = self.SegmentationLossMSE(batch_size=self.dl_studio.batch_size)
        elif loss_type == "DICE":
            criterion1 = self.SegmentationLossDice(batch_size=self.dl_studio.batch_size)
        elif loss_type == "COMBINED":
            criterion1 = self.SegmentationLossComb(batch_size=self.dl_studio.batch_size,dice_scale=dice_scale)
        optimizer = optim.SGD(net.parameters(), lr=self.dl_studio.learning_rate, momentum=self.dl_studio.momentum)
        start_time = time.perf_counter()

        losses = []  # List to store the loss values

        for epoch in range(self.dl_studio.epochs):
            print("")
            running_loss_segmentation = 0.0
            for i, data in enumerate(self.train_dataloader):
                im_tensor, mask_tensor = data['image'], data['mask_tensor']
                im_tensor = im_tensor.to(self.dl_studio.device)
                mask_tensor = mask_tensor.type(torch.FloatTensor)
                mask_tensor = mask_tensor.to(self.dl_studio.device)
                optimizer.zero_grad()
                output = net(im_tensor)
                #print("custom code output shape " + str(output.size()))
                output = output.squeeze(1)
                #print("custom code squeezed output shape " + str(output.size()))
                #print("custom code mask_tensor shape " + str(output.size()))
                segmentation_loss = criterion1(output, mask_tensor)
                segmentation_loss.backward()
                optimizer.step()
                running_loss_segmentation += segmentation_loss.item()
                if i % 100 == 99:
                    current_time = time.perf_counter()
                    elapsed_time = current_time - start_time
                    avg_loss_segmentation = running_loss_segmentation / float(100)
                    print("[epoch=%d/%d, iter=%4d  elapsed_time=%3d secs]  MSE loss: %.3f" % (epoch+1, self.dl_studio.epochs, i+1, elapsed_time, avg_loss_segmentation))
                    FILE1.write("%.3f\n" % avg_loss_segmentation)
                    FILE1.flush()
                    losses.append(avg_loss_segmentation)  # Append the loss value to the list
                    running_loss_segmentation = 0.0
        FILE1.close()  # Close the file after writing

        # Total number of iterations
        total_iterations = len(losses)

        # Create a list of iteration numbers
        iterations = [100*i for i in range(1, total_iterations + 1)]

        # Plotting the loss curve
        plt.plot(iterations,losses, marker='o', label='Training Loss')
        plt.xlabel('Iterations')
        plt.ylabel('Combined Loss')
        plt.title("Combined Loss Curve; Scale= " + str(dice_scale))
        plt.legend()
        plt.show()

        print("\nFinished Training\n")
        self.save_model(net)

    def save_model(self, model):
        '''
        Save the trained model to a disk file
        '''
        torch.save(model.state_dict(), self.dl_studio.path_saved_model)

    def run_code_for_testing_semantic_segmentation(self, net):
      net.load_state_dict(torch.load(self.dl_studio.path_saved_model))
      batch_size = self.dl_studio.batch_size
      image_size = self.dl_studio.image_size
      with torch.no_grad():
        for i, data in enumerate(self.test_dataloader):
          im_tensor,mask_tensor = data['image'],data['mask_tensor']
          if i % 10 == 0:
            print("\n\n\n\nShowing output for test batch %d: " % (i+1))
            outputs = net(im_tensor)
            ## In the statement below: 1st arg for batch items, 2nd for channels, 3rd and 4th for image size
            for image_idx in range(batch_size):
                for m in range(image_size[0]):
                  for n in range(image_size[1]):
                    if  outputs[image_idx,0,m,n] < 200:
                      outputs[image_idx,0,m,n]  =  0
                    else:
                      outputs[image_idx,0,m,n]  =  255
            display_tensor = torch.zeros(3 * batch_size,3,image_size[0],image_size[1], dtype=float)
            display_tensor[:batch_size,:,:,:] = outputs
            display_tensor[batch_size:2*batch_size,:,:,:] = mask_tensor.unsqueeze(1)
            display_tensor[2*batch_size:3*batch_size,:,:,:] = im_tensor

            self.dl_studio.display_tensor_as_image(
              torchvision.utils.make_grid(display_tensor, nrow=batch_size, normalize=True, padding=2, pad_value=10))

#!/usr/bin/env python

##  adopted from semantic_segmentation.py

import random
import numpy
import torch
import os, sys

from DLStudio import *

dls = DLStudio(
                  dataroot = "./data/",
                  image_size = [64,64],
                  path_saved_model = "./saved_model_combined_100",
                  momentum = 0.9,
                  learning_rate = 1e-4,
                  epochs = 6,
                  batch_size = 4,
                  classes = ('cake', 'dog', 'motorcycle'),
                  use_gpu = True,
              )

segmenter = SemanticSegmentation(
                  dl_studio = dls,
              )

dataserver_train = SemanticSegmentation.CocoDataset(
                          dl_studio = dls,
                          class_labels = ["cake","dog","motorcycle"],
                          img_paths = train_img_paths,
                          img_masks = train_img_masks,
                          mode = 'train'
                        )
segmenter.dataserver_train = dataserver_train

dataserver_val = SemanticSegmentation.CocoDataset(
                          dl_studio = dls,
                          class_labels = ["cake","dog","motorcycle"],
                          img_paths = val_img_paths,
                          img_masks = val_img_masks,
                          mode = 'val'
                        )
segmenter.dataserver_val = dataserver_val

segmenter.load_CocoDataset(dataserver_train, dataserver_val)

model = segmenter.mUnet(skip_connections=True, depth=12)

number_of_learnable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print("\n\nThe number of learnable parameters in the model: %d\n" % number_of_learnable_params)

num_layers = len(list(model.parameters()))
print("\nThe number of layers in the model: %d\n\n" % num_layers)

segmenter.custom_run_code_for_training_for_semantic_segmentation(model,"MSE")

segmenter.run_code_for_testing_semantic_segmentation(model)

segmenter.custom_run_code_for_training_for_semantic_segmentation(model,"DICE")

segmenter.run_code_for_testing_semantic_segmentation(model)

segmenter.run_code_for_testing_semantic_segmentation(model)

segmenter.custom_run_code_for_training_for_semantic_segmentation(model,"COMBINED",dice_scale=100.00)

segmenter.run_code_for_testing_semantic_segmentation(model)